{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "import numpy as np\n",
        "import math\n",
        "import plotly.figure_factory as ff"
      ],
      "metadata": {
        "id": "XVsLFTu3BTMh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z_E5SLPx_Lre"
      },
      "outputs": [],
      "source": [
        "def process(x) :\n",
        "  x_proc = x.reshape(len(x), -1)\n",
        "  x_proc = x_proc.astype('float64')\n",
        "  x_proc = x_proc / 255.0\n",
        "  return x_proc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset = \"fashion_mnist\"):\n",
        "  if dataset == \"fashion_mnist\" :\n",
        "      (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  elif dataset == \"mnist\":\n",
        "      (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  \n",
        "  x_train, x_valid = x_train[:int(len(x_train) * 0.9)], x_train[int(len(x_train) * 0.9):]\n",
        "  y_train, y_valid = y_train[:int(len(y_train) * 0.9)], y_train[int(len(y_train) * 0.9):]\n",
        "\n",
        "  x_train = process(x_train)\n",
        "  x_valid = process(x_valid)\n",
        "  x_test = process(x_test) \n",
        "\n",
        "  k = 10\n",
        "  y_train = np.eye(k)[y_train] # one-hot\n",
        "  y_valid = np.eye(k)[y_valid]\n",
        "  y_test = np.eye(k)[y_test]\n",
        "  \n",
        "  return x_train, y_train, x_valid, y_valid, x_test, y_test"
      ],
      "metadata": {
        "id": "HnPTz9lcBKC3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x) :\n",
        "  return 1. / (1. + np.exp(-x))\n",
        "\n",
        "def tanh(x) :\n",
        "  return (2. / (1. + np.exp(-2.*x))) - 1.\n",
        "\n",
        "def relu(x) : # do not use relu with random\n",
        "  return np.where(x >= 0, x, 0.)\n",
        "\n",
        "def softmax(x) :\n",
        "  x = x - np.max(x, axis=0)\n",
        "  y = np.exp(x)\n",
        "  return y / y.sum(axis=0)"
      ],
      "metadata": {
        "id": "Rdt_7K0gBZQC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class my_nn :\n",
        "  # change the default parameters to the best\n",
        "  def __init__(self, n_feature = 784, n_class = 10, nhl = 1, sz = 4, weight_init = \"random\", act_fun = \"sigmoid\", loss = \"cross_entropy\", \n",
        "               epochs = 1, b_sz = 4, optimizer = \"sgd\", lr = 0.1, mom = 0.9, beta = 0.9, beta1 = 0.9, beta2 = 0.999, epsilon = 0.000001, w_d = 0.005) :\n",
        "    self.n_feature = n_feature\n",
        "    self.n_class = n_class\n",
        "    self.nhl = nhl\n",
        "    self.L = nhl + 1\n",
        "    self.sz = sz\n",
        "    self.weight_init = weight_init\n",
        "    self.act_fun = act_fun\n",
        "    self.loss = loss\n",
        "    self.epochs = epochs\n",
        "    self.b_sz = b_sz\n",
        "    self.optimizer = optimizer\n",
        "    self.lr = lr\n",
        "    self.mom = mom\n",
        "    self.beta = beta\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.epsilon = epsilon\n",
        "    self.w_d = w_d\n",
        "\n",
        "    self.W = [0 for i in range(0, self.L+1, 1)]\n",
        "    self.b = [0 for i in range(0, self.L+1, 1)]\n",
        "\n",
        "    self.d_a = [0 for i in range(0, self.L+1, 1)]\n",
        "    self.d_b = [0 for i in range(0, self.L+1, 1)]\n",
        "    self.d_W = [0 for i in range(0, self.L+1, 1)]\n",
        "\n",
        "    self.a = [0 for i in range(0, self.L+1, 1)]\n",
        "    self.h = [0 for i in range(0, self.L+1, 1)]\n",
        "\n",
        "    self.u_W = [0 for i in range(0, self.L+1, 1)]\n",
        "    self.u_b = [0 for i in range(0, self.L+1, 1)]\n",
        "\n",
        "    self.W_look = [0 for i in range(0, self.L+1, 1)]\n",
        "    self.b_look = [0 for i in range(0, self.L+1, 1)]\n",
        "\n",
        "    self.v_W = [0 for i in range(0, self.L+1, 1)]\n",
        "    self.v_b = [0 for i in range(0, self.L+1, 1)]\n",
        "\n",
        "    self.m_W = [0 for i in range(0, self.L+1, 1)]\n",
        "    self.m_b = [0 for i in range(0, self.L+1, 1)]\n",
        "\n",
        "    self.initialization()\n",
        "\n",
        "  ######################################################\n",
        "\n",
        "  def initialization(self) :\n",
        "    if self.act_fun == \"ReLU\" :\n",
        "      self.W[1] = np.random.randn(self.sz, self.n_feature) * np.sqrt(2.0/self.n_feature)\n",
        "      for i in range(2, self.L, 1) :\n",
        "        self.W[i] = np.random.randn(self.sz, self.sz) * math.sqrt(2.0/self.sz)\n",
        "      self.W[self.L] = np.random.randn(self.n_class, self.sz) * math.sqrt(2.0/self.sz)\n",
        "\n",
        "    elif self.weight_init == \"random\" :\n",
        "      self.W[1] = np.random.randn(self.sz, self.n_feature)\n",
        "      for i in range(2, self.L, 1) :\n",
        "        self.W[i] = np.random.randn(self.sz, self.sz)\n",
        "      self.W[self.L] = np.random.randn(self.n_class, self.sz)\n",
        "\n",
        "    elif self.weight_init == \"Xavier\" :\n",
        "      self.W[1] = np.random.randn(self.sz, self.n_feature) * np.sqrt(2.0/self.n_feature)\n",
        "      for i in range(2, self.L, 1) :\n",
        "        self.W[i] = np.random.randn(self.sz, self.sz) * math.sqrt(2.0/self.sz)\n",
        "      self.W[self.L] = np.random.randn(self.n_class, self.sz) * math.sqrt(2.0/self.sz)\n",
        "    \n",
        "    for i in range(1, self.L, 1) :\n",
        "      self.b[i] = np.zeros((self.sz, 1))\n",
        "    self.b[self.L] = np.zeros((self.n_class, 1))\n",
        "  \n",
        "  #########################################################\n",
        "\n",
        "  def forward_propagation(self, x) :\n",
        "    self.h[0] = x\n",
        "\n",
        "    for i in range(1, self.L, 1) :\n",
        "      self.a[i] = self.b[i] + np.dot(self.W[i], self.h[i-1])\n",
        "\n",
        "      if self.act_fun == \"sigmoid\" :\n",
        "        self.h[i] = sigmoid(self.a[i])\n",
        "      elif self.act_fun == \"tanh\" :\n",
        "        self.h[i] = tanh(self.a[i])\n",
        "      elif self.act_fun == \"ReLU\" :\n",
        "        self.h[i] = relu(self.a[i])\n",
        "    \n",
        "    self.a[self.L] = self.b[self.L] + np.dot(self.W[self.L], self.h[self.L-1])\n",
        "    self.h[self.L] = softmax(self.a[self.L]) # h[L] = y_hat\n",
        "\n",
        "  #########################################################\n",
        "\n",
        "  def back_propagation(self, y) :\n",
        "    if self.loss == \"cross_entropy\" :\n",
        "      self.d_a[self.L] = self.h[self.L] - y\n",
        "    elif self.loss == \"mean_squared_error\" :\n",
        "      self.d_a[self.L] = (self.h[self.L] - y) * (self.h[self.L] * (1. - self.h[self.L]))\n",
        "    \n",
        "    self.d_b[self.L] = np.sum(self.d_a[self.L], axis=1, keepdims=True)\n",
        "    self.d_W[self.L] = np.dot(self.d_a[self.L], self.h[self.L-1].T) + self.w_d * self.W[self.L]\n",
        "    \n",
        "    for i in range(self.L-1, 0, -1) :\n",
        "      d_h_i = np.dot(self.W[i+1].T, self.d_a[i+1])\n",
        "      \n",
        "      if self.act_fun == \"sigmoid\" :\n",
        "        g_dash_a_i = self.h[i] * (1. - self.h[i])\n",
        "      elif self.act_fun == \"tanh\" :\n",
        "        g_dash_a_i = 1. - self.h[i]**2\n",
        "      elif self.act_fun == \"ReLU\" :\n",
        "        g_dash_a_i = np.where(self.h[i] > 0., 1., 0.)\n",
        "      \n",
        "      self.d_a[i] = d_h_i * g_dash_a_i\n",
        "      self.d_b[i] = np.sum(self.d_a[i], axis=1, keepdims=True)\n",
        "      self.d_W[i] = np.dot(self.d_a[i], self.h[i-1].T) + self.w_d * self.W[i]\n",
        "\n",
        "  ############################################################\n",
        "\n",
        "  def nag_forward_propagation(self, x) :\n",
        "    self.h[0] = x\n",
        "\n",
        "    for i in range(1, self.L, 1) :\n",
        "      self.a[i] = self.b_look[i] + np.dot(self.W_look[i], self.h[i-1])\n",
        "\n",
        "      if self.act_fun == \"sigmoid\" :\n",
        "        self.h[i] = sigmoid(self.a[i])\n",
        "      elif self.act_fun == \"tanh\" :\n",
        "        self.h[i] = tanh(self.a[i])\n",
        "      elif self.act_fun == \"ReLU\" :\n",
        "        self.h[i] = relu(self.a[i])\n",
        "    \n",
        "    self.a[self.L] = self.b_look[self.L] + np.dot(self.W_look[self.L], self.h[self.L-1])\n",
        "    self.h[self.L] = softmax(self.a[self.L]) # h[L] = y_hat\n",
        "\n",
        "  #########################################################\n",
        "\n",
        "  def nag_back_propagation(self, y) :\n",
        "    if self.loss == \"cross_entropy\" :\n",
        "      self.d_a[self.L] = self.h[self.L] - y\n",
        "    elif self.loss == \"mean_squared_error\" :\n",
        "      self.d_a[self.L] = (self.h[self.L] - y) * (self.h[self.L] * (1. - self.h[self.L]))\n",
        "    \n",
        "    self.d_b[self.L] = np.sum(self.d_a[self.L], axis=1, keepdims=True)\n",
        "    self.d_W[self.L] = np.dot(self.d_a[self.L], self.h[self.L-1].T) + self.w_d * self.W_look[self.L]\n",
        "    \n",
        "    for i in range(self.L-1, 0, -1) :\n",
        "      d_h_i = np.dot(self.W_look[i+1].T, self.d_a[i+1])\n",
        "      \n",
        "      if self.act_fun == \"sigmoid\" :\n",
        "        g_dash_a_i = self.h[i] * (1. - self.h[i])\n",
        "      elif self.act_fun == \"tanh\" :\n",
        "        g_dash_a_i = 1. - self.h[i]**2\n",
        "      elif self.act_fun == \"ReLU\" :\n",
        "        g_dash_a_i = np.where(self.h[i] > 0., 1., 0.)\n",
        "      \n",
        "      self.d_a[i] = d_h_i * g_dash_a_i\n",
        "      self.d_b[i] = np.sum(self.d_a[i], axis=1, keepdims=True)\n",
        "      self.d_W[i] = np.dot(self.d_a[i], self.h[i-1].T) + self.w_d * self.W_look[i]\n",
        "\n",
        "  ############################################################\n",
        "\n",
        "  def predict_prob(self, x) :\n",
        "    a_temp = [0 for i in range(0, self.L+1, 1)]\n",
        "    h_temp = [0 for i in range(0, self.L+1, 1)]\n",
        "    h_temp[0] = x\n",
        "\n",
        "    for i in range(1, self.L, 1) :\n",
        "      a_temp[i] = self.b[i] + np.dot(self.W[i], h_temp[i-1])\n",
        "\n",
        "      if self.act_fun == \"sigmoid\" :\n",
        "        h_temp[i] = sigmoid(a_temp[i])\n",
        "      elif self.act_fun == \"tanh\" :\n",
        "        h_temp[i] = tanh(a_temp[i])\n",
        "      elif self.act_fun == \"ReLU\" :\n",
        "        h_temp[i] = relu(a_temp[i])\n",
        "    \n",
        "    a_temp[self.L] = self.b[self.L] + np.dot(self.W[self.L], h_temp[self.L-1])\n",
        "    h_temp[self.L] = softmax(a_temp[self.L]) # h[L] = y_hat\n",
        "\n",
        "    return h_temp[self.L].T\n",
        "  \n",
        "  #############################################################\n",
        "\n",
        "  def loss_val(self, y_hat, y) :\n",
        "    loss_val = 0.0\n",
        "    N = y.shape[0]\n",
        "\n",
        "    if self.loss == \"cross_entropy\" :\n",
        "      for i in range(0, N, 1) :\n",
        "        temp_loss = math.log(y_hat[i][y[i].argmax()])\n",
        "        loss_val += temp_loss\n",
        "      \n",
        "      loss_val *= (-1.0/N)\n",
        "    \n",
        "    elif self.loss == \"mean_squared_error\" :\n",
        "      loss_val = np.sum((y - y_hat)**2) / N\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "  ##############################################################\n",
        "\n",
        "  def accuracy(self, y_hat, y) :\n",
        "    N = y.shape[0]\n",
        "    n_correct = 0\n",
        "\n",
        "    for i in range(0, N, 1) :\n",
        "      if y[i].argmax() == y_hat[i].argmax() :\n",
        "        n_correct += 1\n",
        "    \n",
        "    return 100 * n_correct / N\n",
        "\n",
        "  ###############################################################\n",
        "\n",
        "  def sgd(self, X, y) :\n",
        "    t = 0\n",
        "    N = X.shape[0]\n",
        "\n",
        "    while t < self.epochs :\n",
        "      for j in range(0, N, self.b_sz) :\n",
        "        r_idx = j + self.b_sz\n",
        "        if (j + self.b_sz) > N :\n",
        "          r_idx = N\n",
        "        self.forward_propagation(X[j:r_idx].T)\n",
        "        self.back_propagation(y[j:r_idx].T)\n",
        "        \n",
        "        for idx in range(1, self.L+1, 1) :\n",
        "          self.W[idx] = self.W[idx] - (self.lr * self.d_W[idx])\n",
        "          self.b[idx] = self.b[idx] - (self.lr * self.d_b[idx])\n",
        "\n",
        "      t += 1\n",
        "\n",
        "  #################################################################\n",
        "\n",
        "  def mgd(self, X, y) :\n",
        "    t = 0\n",
        "    N = X.shape[0]\n",
        "    n_step = 0\n",
        "\n",
        "    while t < self.epochs :\n",
        "      for j in range(0, N, self.b_sz) :\n",
        "        n_step += 1\n",
        "        r_idx = j + self.b_sz\n",
        "        if (j + self.b_sz) > N :\n",
        "          r_idx = N\n",
        "        self.forward_propagation(X[j:r_idx].T)\n",
        "        self.back_propagation(y[j:r_idx].T)\n",
        "\n",
        "        for idx in range(1, self.L+1, 1) :\n",
        "          if n_step == 1 :\n",
        "            self.u_W[idx] = (self.lr * self.d_W[idx])\n",
        "            self.u_b[idx] = (self.lr * self.d_b[idx])\n",
        "          else :\n",
        "            self.u_W[idx] = (self.mom * self.u_W[idx]) + (self.lr * self.d_W[idx])\n",
        "            self.u_b[idx] = (self.mom * self.u_b[idx]) + (self.lr * self.d_b[idx])\n",
        "          \n",
        "          self.W[idx] = self.W[idx] - self.u_W[idx]\n",
        "          self.b[idx] = self.b[idx] - self.u_b[idx]\n",
        "      \n",
        "      t += 1\n",
        "\n",
        "  ##################################################################\n",
        "\n",
        "  def nagd(self, X, y) :\n",
        "    t = 0\n",
        "    N = X.shape[0]\n",
        "    n_step = 0\n",
        "\n",
        "    while t < self.epochs :\n",
        "      for j in range(0, N, self.b_sz) :\n",
        "        n_step += 1\n",
        "        r_idx = j + self.b_sz\n",
        "        if (j + self.b_sz) > N :\n",
        "          r_idx = N\n",
        "        if n_step == 1 :\n",
        "          self.forward_propagation(X[j:r_idx].T)\n",
        "          self.back_propagation(y[j:r_idx].T)\n",
        "        else :\n",
        "          for idx in range(1, self.L+1, 1) :\n",
        "            self.W_look[idx] = self.W[idx] - (self.mom * self.u_W[idx])\n",
        "            self.b_look[idx] = self.b[idx] - (self.mom * self.u_b[idx])\n",
        "          self.nag_forward_propagation(X[j:r_idx].T)\n",
        "          self.nag_back_propagation(y[j:r_idx].T)\n",
        "\n",
        "        for idx in range(1, self.L+1, 1) :\n",
        "          if n_step == 1 :\n",
        "            self.u_W[idx] = (self.lr * self.d_W[idx])\n",
        "            self.u_b[idx] = (self.lr * self.d_b[idx])\n",
        "          else :\n",
        "            self.u_W[idx] = (self.mom * self.u_W[idx]) + (self.lr * self.d_W[idx])\n",
        "            self.u_b[idx] = (self.mom * self.u_b[idx]) + (self.lr * self.d_b[idx])\n",
        "          \n",
        "          self.W[idx] = self.W[idx] - self.u_W[idx]\n",
        "          self.b[idx] = self.b[idx] - self.u_b[idx]\n",
        "\n",
        "      t += 1\n",
        "\n",
        "  ##############################################################\n",
        "\n",
        "  def rmsprop(self, X, y) :\n",
        "    t = 0\n",
        "    N = X.shape[0]\n",
        "    n_step = 0\n",
        "\n",
        "    while t < self.epochs :\n",
        "      for j in range(0, N, self.b_sz) :\n",
        "        n_step += 1\n",
        "        r_idx = j + self.b_sz\n",
        "        if (j + self.b_sz) > N :\n",
        "          r_idx = N\n",
        "        self.forward_propagation(X[j:r_idx].T)\n",
        "        self.back_propagation(y[j:r_idx].T)\n",
        "\n",
        "        for idx in range(1, self.L+1, 1) :\n",
        "          if n_step == 1 :\n",
        "            self.v_W[idx] = ((1. - self.beta) * (self.d_W[idx]**2))\n",
        "            self.v_b[idx] = ((1. - self.beta) * (self.d_b[idx]**2))\n",
        "          else :\n",
        "            self.v_W[idx] = (self.beta * self.v_W[idx]) + ((1. - self.beta) * (self.d_W[idx]**2))\n",
        "            self.v_b[idx] = (self.beta * self.v_b[idx]) + ((1. - self.beta) * (self.d_b[idx]**2))\n",
        "          \n",
        "          self.W[idx] = self.W[idx] - (self.lr / (np.sqrt(self.v_W[idx] + self.epsilon))) * self.d_W[idx]\n",
        "          self.b[idx] = self.b[idx] - (self.lr / (np.sqrt(self.v_b[idx] + self.epsilon))) * self.d_b[idx]\n",
        "      \n",
        "      t += 1\n",
        "  \n",
        "  ##############################################################\n",
        "\n",
        "  def adam(self, X, y) :\n",
        "    t = 0\n",
        "    N = X.shape[0]\n",
        "    n_step = 0\n",
        "\n",
        "    while t < self.epochs :\n",
        "      for j in range(0, N, self.b_sz) :\n",
        "        n_step += 1\n",
        "        r_idx = j + self.b_sz\n",
        "        if (j + self.b_sz) > N :\n",
        "          r_idx = N\n",
        "        self.forward_propagation(X[j:r_idx].T)\n",
        "        self.back_propagation(y[j:r_idx].T)\n",
        "\n",
        "        for idx in range(1, self.L+1, 1) :\n",
        "          if n_step == 1 :\n",
        "            self.m_W[idx] = ((1. - self.beta1) * self.d_W[idx])\n",
        "            self.m_b[idx] = ((1. - self.beta1) * self.d_b[idx])\n",
        "\n",
        "            self.v_W[idx] = ((1. - self.beta2) * (self.d_W[idx]**2))\n",
        "            self.v_b[idx] = ((1. - self.beta2) * (self.d_b[idx]**2))\n",
        "          else :\n",
        "            self.m_W[idx] = (self.beta1 * self.m_W[idx]) + ((1. - self.beta1) * self.d_W[idx])\n",
        "            self.m_b[idx] = (self.beta1 * self.m_b[idx]) + ((1. - self.beta1) * self.d_b[idx])\n",
        "\n",
        "            self.v_W[idx] = (self.beta2 * self.v_W[idx]) + ((1. - self.beta2) * (self.d_W[idx]**2))\n",
        "            self.v_b[idx] = (self.beta2 * self.v_b[idx]) + ((1. - self.beta2) * (self.d_b[idx]**2))\n",
        "          \n",
        "          self.W[idx] = self.W[idx] - (self.lr / (np.sqrt(self.v_W[idx] / (1. - self.beta2**n_step) + self.epsilon))) * (self.m_W[idx] / (1. - self.beta1**n_step))\n",
        "          self.b[idx] = self.b[idx] - (self.lr / (np.sqrt(self.v_b[idx] / (1. - self.beta2**n_step) + self.epsilon))) * (self.m_b[idx] / (1. - self.beta1**n_step))\n",
        "      \n",
        "      t += 1\n",
        "\n",
        "  ##############################################################\n",
        "\n",
        "  def nadam(self, X, y) :\n",
        "    t = 0\n",
        "    N = X.shape[0]\n",
        "    n_step = 0\n",
        "\n",
        "    while t < self.epochs :\n",
        "      for j in range(0, N, self.b_sz) :\n",
        "        n_step += 1\n",
        "        r_idx = j + self.b_sz\n",
        "        if (j + self.b_sz) > N :\n",
        "          r_idx = N\n",
        "        self.forward_propagation(X[j:r_idx].T)\n",
        "        self.back_propagation(y[j:r_idx].T)\n",
        "\n",
        "        for idx in range(1, self.L+1, 1) :\n",
        "          if n_step == 1 :\n",
        "            self.m_W[idx] = ((1. - self.beta1) * self.d_W[idx])\n",
        "            self.m_b[idx] = ((1. - self.beta1) * self.d_b[idx])\n",
        "\n",
        "            self.v_W[idx] = ((1. - self.beta2) * (self.d_W[idx]**2))\n",
        "            self.v_b[idx] = ((1. - self.beta2) * (self.d_b[idx]**2))\n",
        "          else :\n",
        "            self.m_W[idx] = (self.beta1 * self.m_W[idx]) + ((1. - self.beta1) * self.d_W[idx])\n",
        "            self.m_b[idx] = (self.beta1 * self.m_b[idx]) + ((1. - self.beta1) * self.d_b[idx])\n",
        "\n",
        "            self.v_W[idx] = (self.beta2 * self.v_W[idx]) + ((1. - self.beta2) * (self.d_W[idx]**2))\n",
        "            self.v_b[idx] = (self.beta2 * self.v_b[idx]) + ((1. - self.beta2) * (self.d_b[idx]**2))\n",
        "          \n",
        "          W_term = (self.beta1 / (1. - self.beta1**n_step)) * self.m_W[idx]  + ((1. - self.beta1) / (1. - self.beta1**n_step)) * self.d_W[idx]\n",
        "          b_term = (self.beta1 / (1. - self.beta1**n_step)) * self.m_b[idx]  + ((1. - self.beta1) / (1. - self.beta1**n_step)) * self.d_b[idx]\n",
        "\n",
        "          self.W[idx] = self.W[idx] - (self.lr / (np.sqrt(self.v_W[idx] / (1. - self.beta2**n_step) + self.epsilon))) * W_term\n",
        "          self.b[idx] = self.b[idx] - (self.lr / (np.sqrt(self.v_b[idx] / (1. - self.beta2**n_step) + self.epsilon))) * b_term\n",
        "      \n",
        "      t += 1\n",
        "\n",
        "  ##############################################################\n",
        "\n",
        "  def train(self, X_train, y_train) :\n",
        "    if self.optimizer == \"sgd\" :\n",
        "      self.sgd(X_train, y_train)\n",
        "    elif self.optimizer == \"momentum\" :\n",
        "      self.mgd(X_train, y_train)\n",
        "    elif self.optimizer == \"nag\" :\n",
        "      self.nagd(X_train, y_train)\n",
        "    elif self.optimizer == \"rmsprop\" :\n",
        "      self.rmsprop(X_train, y_train)\n",
        "    elif self.optimizer == \"adam\" :\n",
        "      self.adam(X_train, y_train)\n",
        "    elif self.optimizer == \"nadam\" :\n",
        "      self.nadam(X_train, y_train)"
      ],
      "metadata": {
        "id": "XN88sk1-BdCr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(\"mnist\")"
      ],
      "metadata": {
        "id": "HcSUGmEuC4o6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd1b9188-4358-4c02-9efa-58e01bcd52d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters for the best model identified in Q4\n",
        "epochs = 10\n",
        "nhl = 3\n",
        "sz = 128\n",
        "w_d = 0.0\n",
        "lr = 0.001\n",
        "optimizer = \"sgd\"\n",
        "b_sz = 32\n",
        "weight_init = \"Xavier\"\n",
        "act_fun = \"ReLU\""
      ],
      "metadata": {
        "id": "D0obm2eJDEBu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Building the model with the best hyperparameters...')\n",
        "nn_model = my_nn(nhl=nhl, sz=sz, weight_init=weight_init, act_fun=act_fun, epochs=epochs, b_sz=b_sz, optimizer=optimizer, lr=lr, w_d=w_d)\n",
        "nn_model.train(x_train, y_train)\n",
        "print('Model built successfully.')\n",
        "\n",
        "y_test_hat = nn_model.predict_prob(x_test.T)\n",
        "test_acc = nn_model.accuracy(y_test_hat, y_test)\n",
        "print('Accuracy on test set = ', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw5DO_t_Elmn",
        "outputId": "6f465823-b802-4ea6-8a75-9c52425e2151"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the model with the best hyperparameters...\n",
            "Model built successfully.\n",
            "Accuracy on test set =  97.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters for the 2nd best model identified in Q4\n",
        "epochs = 10\n",
        "nhl = 3\n",
        "sz = 128\n",
        "w_d = 0.0005\n",
        "lr = 0.0001\n",
        "optimizer = \"rmsprop\"\n",
        "b_sz = 32\n",
        "weight_init = \"Xavier\"\n",
        "act_fun = \"ReLU\""
      ],
      "metadata": {
        "id": "8V7kxwjIQe0v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Building the model with the 2nd best hyperparameters...')\n",
        "nn_model = my_nn(nhl=nhl, sz=sz, weight_init=weight_init, act_fun=act_fun, epochs=epochs, b_sz=b_sz, optimizer=optimizer, lr=lr, w_d=w_d)\n",
        "nn_model.train(x_train, y_train)\n",
        "print('Model built successfully.')\n",
        "\n",
        "y_test_hat = nn_model.predict_prob(x_test.T)\n",
        "test_acc = nn_model.accuracy(y_test_hat, y_test)\n",
        "print('Accuracy on test set = ', test_acc)"
      ],
      "metadata": {
        "id": "M3NRdnwfOFVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa8e189-c330-4b86-8723-9846ac5e2741"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the model with the 2nd best hyperparameters...\n",
            "Model built successfully.\n",
            "Accuracy on test set =  97.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters for the 3rd best model identified in Q4\n",
        "epochs = 10\n",
        "nhl = 4\n",
        "sz = 128\n",
        "w_d = 0.0\n",
        "lr = 0.0001\n",
        "optimizer = \"nag\"\n",
        "b_sz = 32\n",
        "weight_init = \"random\"\n",
        "act_fun = \"ReLU\""
      ],
      "metadata": {
        "id": "xrpiQscMRo9p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Building the model with the 3rd best hyperparameters...')\n",
        "nn_model = my_nn(nhl=nhl, sz=sz, weight_init=weight_init, act_fun=act_fun, epochs=epochs, b_sz=b_sz, optimizer=optimizer, lr=lr, w_d=w_d)\n",
        "nn_model.train(x_train, y_train)\n",
        "print('Model built successfully.')\n",
        "\n",
        "y_test_hat = nn_model.predict_prob(x_test.T)\n",
        "test_acc = nn_model.accuracy(y_test_hat, y_test)\n",
        "print('Accuracy on test set = ', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUxosUQoRqQf",
        "outputId": "d73eedb0-f809-49f2-9d8f-b6739151ebc0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the model with the 3rd best hyperparameters...\n",
            "Model built successfully.\n",
            "Accuracy on test set =  97.03\n"
          ]
        }
      ]
    }
  ]
}