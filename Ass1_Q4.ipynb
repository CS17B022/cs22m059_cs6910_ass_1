{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JT8qGV2QLirH"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist, mnist\n",
    "import numpy as np\n",
    "import math\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LmQ05YTRnT8A"
   },
   "outputs": [],
   "source": [
    "def process(x) :\n",
    "  x_proc = x.reshape(len(x), -1)\n",
    "  x_proc = x_proc.astype('float64')\n",
    "  x_proc = x_proc / 255.0\n",
    "  return x_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nrXeIiRKljjq"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset = \"fashion_mnist\"):\n",
    "  if dataset == \"fashion_mnist\" :\n",
    "      (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "  elif dataset == \"mnist\":\n",
    "      (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "  \n",
    "  x_train, x_valid = x_train[:int(len(x_train) * 0.9)], x_train[int(len(x_train) * 0.9):]\n",
    "  y_train, y_valid = y_train[:int(len(y_train) * 0.9)], y_train[int(len(y_train) * 0.9):]\n",
    "\n",
    "  x_train = process(x_train)\n",
    "  x_valid = process(x_valid)\n",
    "  x_test = process(x_test) \n",
    "\n",
    "  k = 10\n",
    "  y_train = np.eye(k)[y_train] # one-hot\n",
    "  y_valid = np.eye(k)[y_valid]\n",
    "  y_test = np.eye(k)[y_test]\n",
    "  \n",
    "  return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bnV_DU2LLuYL"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x) :\n",
    "  return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def tanh(x) :\n",
    "  return (2. / (1. + np.exp(-2.*x))) - 1.\n",
    "\n",
    "def relu(x) : # do not use relu with random\n",
    "  return np.where(x >= 0, x, 0.)\n",
    "\n",
    "def softmax(x) :\n",
    "  x = x - np.max(x, axis=0)\n",
    "  y = np.exp(x)\n",
    "  return y / y.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8iZyB60QLXAh"
   },
   "outputs": [],
   "source": [
    "class my_nn :\n",
    "\n",
    "  def __init__(self, n_feature = 784, n_class = 10, nhl = 1, sz = 4, weight_init = \"random\", act_fun = \"sigmoid\", loss = \"cross_entropy\", \n",
    "               epochs = 1, b_sz = 4, optimizer = \"sgd\", lr = 0.1, mom = 0.9, beta = 0.9, beta1 = 0.9, beta2 = 0.999, epsilon = 0.000001, w_d = 0.005) :\n",
    "    self.n_feature = n_feature\n",
    "    self.n_class = n_class\n",
    "    self.nhl = nhl\n",
    "    self.L = nhl + 1\n",
    "    self.sz = sz\n",
    "    self.weight_init = weight_init\n",
    "    self.act_fun = act_fun\n",
    "    self.loss = loss\n",
    "    self.epochs = epochs\n",
    "    self.b_sz = b_sz\n",
    "    self.optimizer = optimizer\n",
    "    self.lr = lr\n",
    "    self.mom = mom\n",
    "    self.beta = beta\n",
    "    self.beta1 = beta1\n",
    "    self.beta2 = beta2\n",
    "    self.epsilon = epsilon\n",
    "    self.w_d = w_d\n",
    "\n",
    "    self.W = [0 for i in range(0, self.L+1, 1)]\n",
    "    self.b = [0 for i in range(0, self.L+1, 1)]\n",
    "\n",
    "    self.d_a = [0 for i in range(0, self.L+1, 1)]\n",
    "    self.d_b = [0 for i in range(0, self.L+1, 1)]\n",
    "    self.d_W = [0 for i in range(0, self.L+1, 1)]\n",
    "\n",
    "    self.a = [0 for i in range(0, self.L+1, 1)]\n",
    "    self.h = [0 for i in range(0, self.L+1, 1)]\n",
    "\n",
    "    self.u_W = [0 for i in range(0, self.L+1, 1)]\n",
    "    self.u_b = [0 for i in range(0, self.L+1, 1)]\n",
    "\n",
    "    self.W_look = [0 for i in range(0, self.L+1, 1)]\n",
    "    self.b_look = [0 for i in range(0, self.L+1, 1)]\n",
    "\n",
    "    self.v_W = [0 for i in range(0, self.L+1, 1)]\n",
    "    self.v_b = [0 for i in range(0, self.L+1, 1)]\n",
    "\n",
    "    self.m_W = [0 for i in range(0, self.L+1, 1)]\n",
    "    self.m_b = [0 for i in range(0, self.L+1, 1)]\n",
    "\n",
    "    self.initialization()\n",
    "\n",
    "  ######################################################\n",
    "\n",
    "  def initialization(self) :\n",
    "    if self.act_fun == \"ReLU\" :\n",
    "      self.W[1] = np.random.randn(self.sz, self.n_feature) * np.sqrt(2.0/self.n_feature)\n",
    "      for i in range(2, self.L, 1) :\n",
    "        self.W[i] = np.random.randn(self.sz, self.sz) * math.sqrt(2.0/self.sz)\n",
    "      self.W[self.L] = np.random.randn(self.n_class, self.sz) * math.sqrt(2.0/self.sz)\n",
    "\n",
    "    elif self.weight_init == \"random\" :\n",
    "      self.W[1] = np.random.randn(self.sz, self.n_feature)\n",
    "      for i in range(2, self.L, 1) :\n",
    "        self.W[i] = np.random.randn(self.sz, self.sz)\n",
    "      self.W[self.L] = np.random.randn(self.n_class, self.sz)\n",
    "\n",
    "    elif self.weight_init == \"Xavier\" :\n",
    "      self.W[1] = np.random.randn(self.sz, self.n_feature) * np.sqrt(2.0/self.n_feature)\n",
    "      for i in range(2, self.L, 1) :\n",
    "        self.W[i] = np.random.randn(self.sz, self.sz) * math.sqrt(2.0/self.sz)\n",
    "      self.W[self.L] = np.random.randn(self.n_class, self.sz) * math.sqrt(2.0/self.sz)\n",
    "    \n",
    "    for i in range(1, self.L, 1) :\n",
    "      self.b[i] = np.zeros((self.sz, 1))\n",
    "    self.b[self.L] = np.zeros((self.n_class, 1))\n",
    "  \n",
    "  #########################################################\n",
    "\n",
    "  def forward_propagation(self, x) :\n",
    "    self.h[0] = x\n",
    "\n",
    "    for i in range(1, self.L, 1) :\n",
    "      self.a[i] = self.b[i] + np.dot(self.W[i], self.h[i-1])\n",
    "\n",
    "      if self.act_fun == \"sigmoid\" :\n",
    "        self.h[i] = sigmoid(self.a[i])\n",
    "      elif self.act_fun == \"tanh\" :\n",
    "        self.h[i] = tanh(self.a[i])\n",
    "      elif self.act_fun == \"ReLU\" :\n",
    "        self.h[i] = relu(self.a[i])\n",
    "    \n",
    "    self.a[self.L] = self.b[self.L] + np.dot(self.W[self.L], self.h[self.L-1])\n",
    "    self.h[self.L] = softmax(self.a[self.L]) # h[L] = y_hat\n",
    "\n",
    "  #########################################################\n",
    "\n",
    "  def back_propagation(self, y) :\n",
    "    if self.loss == \"cross_entropy\" :\n",
    "      self.d_a[self.L] = self.h[self.L] - y\n",
    "    elif self.loss == \"mean_squared_error\" :\n",
    "      self.d_a[self.L] = (self.h[self.L] - y) * (self.h[self.L] * (1. - self.h[self.L]))\n",
    "    \n",
    "    self.d_b[self.L] = np.sum(self.d_a[self.L], axis=1, keepdims=True)\n",
    "    self.d_W[self.L] = np.dot(self.d_a[self.L], self.h[self.L-1].T) + self.w_d * self.W[self.L]\n",
    "    \n",
    "    for i in range(self.L-1, 0, -1) :\n",
    "      d_h_i = np.dot(self.W[i+1].T, self.d_a[i+1])\n",
    "      \n",
    "      if self.act_fun == \"sigmoid\" :\n",
    "        g_dash_a_i = self.h[i] * (1. - self.h[i])\n",
    "      elif self.act_fun == \"tanh\" :\n",
    "        g_dash_a_i = 1. - self.h[i]**2\n",
    "      elif self.act_fun == \"ReLU\" :\n",
    "        g_dash_a_i = np.where(self.h[i] > 0., 1., 0.)\n",
    "      \n",
    "      self.d_a[i] = d_h_i * g_dash_a_i\n",
    "      self.d_b[i] = np.sum(self.d_a[i], axis=1, keepdims=True)\n",
    "      self.d_W[i] = np.dot(self.d_a[i], self.h[i-1].T) + self.w_d * self.W[i]\n",
    "\n",
    "  ############################################################\n",
    "\n",
    "  def nag_forward_propagation(self, x) :\n",
    "    self.h[0] = x\n",
    "\n",
    "    for i in range(1, self.L, 1) :\n",
    "      self.a[i] = self.b_look[i] + np.dot(self.W_look[i], self.h[i-1])\n",
    "\n",
    "      if self.act_fun == \"sigmoid\" :\n",
    "        self.h[i] = sigmoid(self.a[i])\n",
    "      elif self.act_fun == \"tanh\" :\n",
    "        self.h[i] = tanh(self.a[i])\n",
    "      elif self.act_fun == \"ReLU\" :\n",
    "        self.h[i] = relu(self.a[i])\n",
    "    \n",
    "    self.a[self.L] = self.b_look[self.L] + np.dot(self.W_look[self.L], self.h[self.L-1])\n",
    "    self.h[self.L] = softmax(self.a[self.L]) # h[L] = y_hat\n",
    "\n",
    "  #########################################################\n",
    "\n",
    "  def nag_back_propagation(self, y) :\n",
    "    if self.loss == \"cross_entropy\" :\n",
    "      self.d_a[self.L] = self.h[self.L] - y\n",
    "    elif self.loss == \"mean_squared_error\" :\n",
    "      self.d_a[self.L] = (self.h[self.L] - y) * (self.h[self.L] * (1. - self.h[self.L]))\n",
    "    \n",
    "    self.d_b[self.L] = np.sum(self.d_a[self.L], axis=1, keepdims=True)\n",
    "    self.d_W[self.L] = np.dot(self.d_a[self.L], self.h[self.L-1].T) + self.w_d * self.W_look[self.L]\n",
    "    \n",
    "    for i in range(self.L-1, 0, -1) :\n",
    "      d_h_i = np.dot(self.W_look[i+1].T, self.d_a[i+1])\n",
    "      \n",
    "      if self.act_fun == \"sigmoid\" :\n",
    "        g_dash_a_i = self.h[i] * (1. - self.h[i])\n",
    "      elif self.act_fun == \"tanh\" :\n",
    "        g_dash_a_i = 1. - self.h[i]**2\n",
    "      elif self.act_fun == \"ReLU\" :\n",
    "        g_dash_a_i = np.where(self.h[i] > 0., 1., 0.)\n",
    "      \n",
    "      self.d_a[i] = d_h_i * g_dash_a_i\n",
    "      self.d_b[i] = np.sum(self.d_a[i], axis=1, keepdims=True)\n",
    "      self.d_W[i] = np.dot(self.d_a[i], self.h[i-1].T) + self.w_d * self.W_look[i]\n",
    "\n",
    "  ############################################################\n",
    "\n",
    "  def predict_prob(self, x) :\n",
    "    a_temp = [0 for i in range(0, self.L+1, 1)]\n",
    "    h_temp = [0 for i in range(0, self.L+1, 1)]\n",
    "    h_temp[0] = x\n",
    "\n",
    "    for i in range(1, self.L, 1) :\n",
    "      a_temp[i] = self.b[i] + np.dot(self.W[i], h_temp[i-1])\n",
    "\n",
    "      if self.act_fun == \"sigmoid\" :\n",
    "        h_temp[i] = sigmoid(a_temp[i])\n",
    "      elif self.act_fun == \"tanh\" :\n",
    "        h_temp[i] = tanh(a_temp[i])\n",
    "      elif self.act_fun == \"ReLU\" :\n",
    "        h_temp[i] = relu(a_temp[i])\n",
    "    \n",
    "    a_temp[self.L] = self.b[self.L] + np.dot(self.W[self.L], h_temp[self.L-1])\n",
    "    h_temp[self.L] = softmax(a_temp[self.L]) # h[L] = y_hat\n",
    "\n",
    "    return h_temp[self.L].T\n",
    "  \n",
    "  #############################################################\n",
    "\n",
    "  def loss_val(self, y_hat, y) :\n",
    "    loss_val = 0.0\n",
    "    N = y.shape[0]\n",
    "\n",
    "    if self.loss == \"cross_entropy\" :\n",
    "      for i in range(0, N, 1) :\n",
    "        temp_loss = math.log(y_hat[i][y[i].argmax()])\n",
    "        loss_val += temp_loss\n",
    "      \n",
    "      loss_val *= (-1.0/N)\n",
    "    \n",
    "    elif self.loss == \"mean_squared_error\" :\n",
    "      loss_val = np.sum((y - y_hat)**2) / N\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "  ##############################################################\n",
    "\n",
    "  def accuracy(self, y_hat, y) :\n",
    "    N = y.shape[0]\n",
    "    n_correct = 0\n",
    "\n",
    "    for i in range(0, N, 1) :\n",
    "      if y[i].argmax() == y_hat[i].argmax() :\n",
    "        n_correct += 1\n",
    "    \n",
    "    return 100 * n_correct / N\n",
    "\n",
    "  ###############################################################\n",
    "\n",
    "  def sgd(self, X, y, X_valid, y_valid) :\n",
    "    t = 0\n",
    "    N = X.shape[0]\n",
    "\n",
    "    while t < self.epochs :\n",
    "      for j in range(0, N, self.b_sz) :\n",
    "        r_idx = j + self.b_sz\n",
    "        if (j + self.b_sz) > N :\n",
    "          r_idx = N\n",
    "        self.forward_propagation(X[j:r_idx].T)\n",
    "        self.back_propagation(y[j:r_idx].T)\n",
    "        \n",
    "        for idx in range(1, self.L+1, 1) :\n",
    "          self.W[idx] = self.W[idx] - (self.lr * self.d_W[idx])\n",
    "          self.b[idx] = self.b[idx] - (self.lr * self.d_b[idx])\n",
    "      \n",
    "      y_hat = self.predict_prob(X.T)\n",
    "      tr_loss = self.loss_val(y_hat, y)\n",
    "      tr_acc = self.accuracy(y_hat, y)\n",
    "\n",
    "      y_val_hat = self.predict_prob(X_valid.T)\n",
    "      val_loss = self.loss_val(y_val_hat, y_valid)\n",
    "      val_acc = self.accuracy(y_val_hat, y_valid)\n",
    "\n",
    "      print(f\"epoch {t + 1} : train_loss = {tr_loss:.2f} valid_loss = {val_loss:.2f}, train accuracy = {tr_acc:.2f} valid_accuracy = {val_acc:.2f}\")\n",
    "      wandb.log({'tr_loss' : tr_loss, 'tr_accuracy' : tr_acc, 'val_loss' : val_loss, 'val_accuracy' : val_acc})\n",
    "\n",
    "      t += 1\n",
    "\n",
    "  #################################################################\n",
    "\n",
    "  def mgd(self, X, y, X_valid, y_valid) :\n",
    "    t = 0\n",
    "    N = X.shape[0]\n",
    "    n_step = 0\n",
    "\n",
    "    while t < self.epochs :\n",
    "      for j in range(0, N, self.b_sz) :\n",
    "        n_step += 1\n",
    "        r_idx = j + self.b_sz\n",
    "        if (j + self.b_sz) > N :\n",
    "          r_idx = N\n",
    "        self.forward_propagation(X[j:r_idx].T)\n",
    "        self.back_propagation(y[j:r_idx].T)\n",
    "\n",
    "        for idx in range(1, self.L+1, 1) :\n",
    "          if n_step == 1 :\n",
    "            self.u_W[idx] = (self.lr * self.d_W[idx])\n",
    "            self.u_b[idx] = (self.lr * self.d_b[idx])\n",
    "          else :\n",
    "            self.u_W[idx] = (self.mom * self.u_W[idx]) + (self.lr * self.d_W[idx])\n",
    "            self.u_b[idx] = (self.mom * self.u_b[idx]) + (self.lr * self.d_b[idx])\n",
    "          \n",
    "          self.W[idx] = self.W[idx] - self.u_W[idx]\n",
    "          self.b[idx] = self.b[idx] - self.u_b[idx]\n",
    "\n",
    "      y_hat = self.predict_prob(X.T)\n",
    "      tr_loss = self.loss_val(y_hat, y)\n",
    "      tr_acc = self.accuracy(y_hat, y)\n",
    "\n",
    "      y_val_hat = self.predict_prob(X_valid.T)\n",
    "      val_loss = self.loss_val(y_val_hat, y_valid)\n",
    "      val_acc = self.accuracy(y_val_hat, y_valid)\n",
    "\n",
    "      print(f\"epoch {t + 1} : train_loss = {tr_loss:.2f} valid_loss = {val_loss:.2f}, train accuracy = {tr_acc:.2f} valid_accuracy = {val_acc:.2f}\")\n",
    "      wandb.log({'tr_loss' : tr_loss, 'tr_accuracy' : tr_acc, 'val_loss' : val_loss, 'val_accuracy' : val_acc})\n",
    "      \n",
    "      t += 1\n",
    "\n",
    "  ##################################################################\n",
    "\n",
    "  def nagd(self, X, y, X_valid, y_valid) :\n",
    "    t = 0\n",
    "    N = X.shape[0]\n",
    "    n_step = 0\n",
    "\n",
    "    while t < self.epochs :\n",
    "      for j in range(0, N, self.b_sz) :\n",
    "        n_step += 1\n",
    "        r_idx = j + self.b_sz\n",
    "        if (j + self.b_sz) > N :\n",
    "          r_idx = N\n",
    "        if n_step == 1 :\n",
    "          self.forward_propagation(X[j:r_idx].T)\n",
    "          self.back_propagation(y[j:r_idx].T)\n",
    "        else :\n",
    "          for idx in range(1, self.L+1, 1) :\n",
    "            self.W_look[idx] = self.W[idx] - (self.mom * self.u_W[idx])\n",
    "            self.b_look[idx] = self.b[idx] - (self.mom * self.u_b[idx])\n",
    "          self.nag_forward_propagation(X[j:r_idx].T)\n",
    "          self.nag_back_propagation(y[j:r_idx].T)\n",
    "\n",
    "        for idx in range(1, self.L+1, 1) :\n",
    "          if n_step == 1 :\n",
    "            self.u_W[idx] = (self.lr * self.d_W[idx])\n",
    "            self.u_b[idx] = (self.lr * self.d_b[idx])\n",
    "          else :\n",
    "            self.u_W[idx] = (self.mom * self.u_W[idx]) + (self.lr * self.d_W[idx])\n",
    "            self.u_b[idx] = (self.mom * self.u_b[idx]) + (self.lr * self.d_b[idx])\n",
    "          \n",
    "          self.W[idx] = self.W[idx] - self.u_W[idx]\n",
    "          self.b[idx] = self.b[idx] - self.u_b[idx]\n",
    "        \n",
    "      y_hat = self.predict_prob(X.T)\n",
    "      tr_loss = self.loss_val(y_hat, y)\n",
    "      tr_acc = self.accuracy(y_hat, y)\n",
    "\n",
    "      y_val_hat = self.predict_prob(X_valid.T)\n",
    "      val_loss = self.loss_val(y_val_hat, y_valid)\n",
    "      val_acc = self.accuracy(y_val_hat, y_valid)\n",
    "\n",
    "      print(f\"epoch {t + 1} : train_loss = {tr_loss:.2f} valid_loss = {val_loss:.2f}, train accuracy = {tr_acc:.2f} valid_accuracy = {val_acc:.2f}\")\n",
    "      wandb.log({'tr_loss' : tr_loss, 'tr_accuracy' : tr_acc, 'val_loss' : val_loss, 'val_accuracy' : val_acc})\n",
    "      t += 1\n",
    "\n",
    "  ##############################################################\n",
    "\n",
    "  def rmsprop(self, X, y, X_valid, y_valid) :\n",
    "    t = 0\n",
    "    N = X.shape[0]\n",
    "    n_step = 0\n",
    "\n",
    "    while t < self.epochs :\n",
    "      for j in range(0, N, self.b_sz) :\n",
    "        n_step += 1\n",
    "        r_idx = j + self.b_sz\n",
    "        if (j + self.b_sz) > N :\n",
    "          r_idx = N\n",
    "        self.forward_propagation(X[j:r_idx].T)\n",
    "        self.back_propagation(y[j:r_idx].T)\n",
    "\n",
    "        for idx in range(1, self.L+1, 1) :\n",
    "          if n_step == 1 :\n",
    "            self.v_W[idx] = ((1. - self.beta) * (self.d_W[idx]**2))\n",
    "            self.v_b[idx] = ((1. - self.beta) * (self.d_b[idx]**2))\n",
    "          else :\n",
    "            self.v_W[idx] = (self.beta * self.v_W[idx]) + ((1. - self.beta) * (self.d_W[idx]**2))\n",
    "            self.v_b[idx] = (self.beta * self.v_b[idx]) + ((1. - self.beta) * (self.d_b[idx]**2))\n",
    "          \n",
    "          self.W[idx] = self.W[idx] - (self.lr / (np.sqrt(self.v_W[idx] + self.epsilon))) * self.d_W[idx]\n",
    "          self.b[idx] = self.b[idx] - (self.lr / (np.sqrt(self.v_b[idx] + self.epsilon))) * self.d_b[idx]\n",
    "        \n",
    "      y_hat = self.predict_prob(X.T)\n",
    "      tr_loss = self.loss_val(y_hat, y)\n",
    "      tr_acc = self.accuracy(y_hat, y)\n",
    "\n",
    "      y_val_hat = self.predict_prob(X_valid.T)\n",
    "      val_loss = self.loss_val(y_val_hat, y_valid)\n",
    "      val_acc = self.accuracy(y_val_hat, y_valid)\n",
    "\n",
    "      print(f\"epoch {t + 1} : train_loss = {tr_loss:.2f} valid_loss = {val_loss:.2f}, train accuracy = {tr_acc:.2f} valid_accuracy = {val_acc:.2f}\")\n",
    "      wandb.log({'tr_loss' : tr_loss, 'tr_accuracy' : tr_acc, 'val_loss' : val_loss, 'val_accuracy' : val_acc})\n",
    "      t += 1\n",
    "  \n",
    "  ##############################################################\n",
    "\n",
    "  def adam(self, X, y, X_valid, y_valid) :\n",
    "    t = 0\n",
    "    N = X.shape[0]\n",
    "    n_step = 0\n",
    "\n",
    "    while t < self.epochs :\n",
    "      for j in range(0, N, self.b_sz) :\n",
    "        n_step += 1\n",
    "        r_idx = j + self.b_sz\n",
    "        if (j + self.b_sz) > N :\n",
    "          r_idx = N\n",
    "        self.forward_propagation(X[j:r_idx].T)\n",
    "        self.back_propagation(y[j:r_idx].T)\n",
    "\n",
    "        for idx in range(1, self.L+1, 1) :\n",
    "          if n_step == 1 :\n",
    "            self.m_W[idx] = ((1. - self.beta1) * self.d_W[idx])\n",
    "            self.m_b[idx] = ((1. - self.beta1) * self.d_b[idx])\n",
    "\n",
    "            self.v_W[idx] = ((1. - self.beta2) * (self.d_W[idx]**2))\n",
    "            self.v_b[idx] = ((1. - self.beta2) * (self.d_b[idx]**2))\n",
    "          else :\n",
    "            self.m_W[idx] = (self.beta1 * self.m_W[idx]) + ((1. - self.beta1) * self.d_W[idx])\n",
    "            self.m_b[idx] = (self.beta1 * self.m_b[idx]) + ((1. - self.beta1) * self.d_b[idx])\n",
    "\n",
    "            self.v_W[idx] = (self.beta2 * self.v_W[idx]) + ((1. - self.beta2) * (self.d_W[idx]**2))\n",
    "            self.v_b[idx] = (self.beta2 * self.v_b[idx]) + ((1. - self.beta2) * (self.d_b[idx]**2))\n",
    "          \n",
    "          self.W[idx] = self.W[idx] - (self.lr / (np.sqrt(self.v_W[idx] / (1. - self.beta2**n_step) + self.epsilon))) * (self.m_W[idx] / (1. - self.beta1**n_step))\n",
    "          self.b[idx] = self.b[idx] - (self.lr / (np.sqrt(self.v_b[idx] / (1. - self.beta2**n_step) + self.epsilon))) * (self.m_b[idx] / (1. - self.beta1**n_step))\n",
    "        \n",
    "      y_hat = self.predict_prob(X.T)\n",
    "      tr_loss = self.loss_val(y_hat, y)\n",
    "      tr_acc = self.accuracy(y_hat, y)\n",
    "\n",
    "      y_val_hat = self.predict_prob(X_valid.T)\n",
    "      val_loss = self.loss_val(y_val_hat, y_valid)\n",
    "      val_acc = self.accuracy(y_val_hat, y_valid)\n",
    "\n",
    "      print(f\"epoch {t + 1} : train_loss = {tr_loss:.2f} valid_loss = {val_loss:.2f}, train accuracy = {tr_acc:.2f} valid_accuracy = {val_acc:.2f}\")\n",
    "      wandb.log({'tr_loss' : tr_loss, 'tr_accuracy' : tr_acc, 'val_loss' : val_loss, 'val_accuracy' : val_acc})\n",
    "      t += 1\n",
    "\n",
    "  ##############################################################\n",
    "\n",
    "  def nadam(self, X, y, X_valid, y_valid) :\n",
    "    t = 0\n",
    "    N = X.shape[0]\n",
    "    n_step = 0\n",
    "\n",
    "    while t < self.epochs :\n",
    "      for j in range(0, N, self.b_sz) :\n",
    "        n_step += 1\n",
    "        r_idx = j + self.b_sz\n",
    "        if (j + self.b_sz) > N :\n",
    "          r_idx = N\n",
    "        self.forward_propagation(X[j:r_idx].T)\n",
    "        self.back_propagation(y[j:r_idx].T)\n",
    "\n",
    "        for idx in range(1, self.L+1, 1) :\n",
    "          if n_step == 1 :\n",
    "            self.m_W[idx] = ((1. - self.beta1) * self.d_W[idx])\n",
    "            self.m_b[idx] = ((1. - self.beta1) * self.d_b[idx])\n",
    "\n",
    "            self.v_W[idx] = ((1. - self.beta2) * (self.d_W[idx]**2))\n",
    "            self.v_b[idx] = ((1. - self.beta2) * (self.d_b[idx]**2))\n",
    "          else :\n",
    "            self.m_W[idx] = (self.beta1 * self.m_W[idx]) + ((1. - self.beta1) * self.d_W[idx])\n",
    "            self.m_b[idx] = (self.beta1 * self.m_b[idx]) + ((1. - self.beta1) * self.d_b[idx])\n",
    "\n",
    "            self.v_W[idx] = (self.beta2 * self.v_W[idx]) + ((1. - self.beta2) * (self.d_W[idx]**2))\n",
    "            self.v_b[idx] = (self.beta2 * self.v_b[idx]) + ((1. - self.beta2) * (self.d_b[idx]**2))\n",
    "          \n",
    "          W_term = (self.beta1 / (1. - self.beta1**n_step)) * self.m_W[idx]  + ((1. - self.beta1) / (1. - self.beta1**n_step)) * self.d_W[idx]\n",
    "          b_term = (self.beta1 / (1. - self.beta1**n_step)) * self.m_b[idx]  + ((1. - self.beta1) / (1. - self.beta1**n_step)) * self.d_b[idx]\n",
    "\n",
    "          self.W[idx] = self.W[idx] - (self.lr / (np.sqrt(self.v_W[idx] / (1. - self.beta2**n_step) + self.epsilon))) * W_term\n",
    "          self.b[idx] = self.b[idx] - (self.lr / (np.sqrt(self.v_b[idx] / (1. - self.beta2**n_step) + self.epsilon))) * b_term\n",
    "        \n",
    "      y_hat = self.predict_prob(X.T)\n",
    "      tr_loss = self.loss_val(y_hat, y)\n",
    "      tr_acc = self.accuracy(y_hat, y)\n",
    "\n",
    "      y_val_hat = self.predict_prob(X_valid.T)\n",
    "      val_loss = self.loss_val(y_val_hat, y_valid)\n",
    "      val_acc = self.accuracy(y_val_hat, y_valid)\n",
    "\n",
    "      print(f\"epoch {t + 1} : train_loss = {tr_loss:.2f} valid_loss = {val_loss:.2f}, train accuracy = {tr_acc:.2f} valid_accuracy = {val_acc:.2f}\")\n",
    "      wandb.log({'tr_loss' : tr_loss, 'tr_accuracy' : tr_acc, 'val_loss' : val_loss, 'val_accuracy' : val_acc})\n",
    "      t += 1\n",
    "\n",
    "  ##############################################################\n",
    "\n",
    "  def train(self, X_train, y_train, X_valid, y_valid) :\n",
    "    if self.optimizer == \"sgd\" :\n",
    "      self.sgd(X_train, y_train, X_valid, y_valid)\n",
    "    elif self.optimizer == \"momentum\" :\n",
    "      self.mgd(X_train, y_train, X_valid, y_valid)\n",
    "    elif self.optimizer == \"nag\" :\n",
    "      self.nagd(X_train, y_train, X_valid, y_valid)\n",
    "    elif self.optimizer == \"rmsprop\" :\n",
    "      self.rmsprop(X_train, y_train, X_valid, y_valid)\n",
    "    elif self.optimizer == \"adam\" :\n",
    "      self.adam(X_train, y_train, X_valid, y_valid)\n",
    "    elif self.optimizer == \"nadam\" :\n",
    "      self.nadam(X_train, y_train, X_valid, y_valid)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9FcMP6QhpVh4"
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8nVOD4unqlVJ"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name' : 'Bayesian_sweep_cross_entropy',\n",
    "    'metric': {\n",
    "      'name': 'valid accuracy',\n",
    "      'goal': 'maximize'  \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {\n",
    "            'values': [5, 10]\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [3, 4, 5]\n",
    "        },\n",
    "         'hidden_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0, 0.0005, 0.5]\n",
    "        },\n",
    "         'learning_rate': {\n",
    "            'values': [0.001, 0.0001]\n",
    "        },\n",
    "         'optimizer': {\n",
    "            'values': ['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam']           \n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "         'weight_init': {\n",
    "            'values': ['random', 'Xavier']            \n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['sigmoid', 'tanh', 'ReLU']\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep = sweep_config, project = 'cs6910_dl_assgn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "r_stHLWosxQa"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  with wandb.init() as run:\n",
    "    run_name = \"opt_\" + wandb.config.optimizer + \"_ac_\" + wandb.config.activation + \"_bs_\" + str(wandb.config.batch_size)\\\n",
    "            + \"_hl_\" + str(wandb.config.num_layers) + \"_lr_\" + str(wandb.config.learning_rate)\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    epochs = wandb.config.epochs\n",
    "    nhl = wandb.config.num_layers\n",
    "    sz = wandb.config.hidden_size\n",
    "    w_d = wandb.config.weight_decay\n",
    "    lr = wandb.config.learning_rate\n",
    "    optimizer = wandb.config.optimizer\n",
    "    b_sz = wandb.config.batch_size\n",
    "    weight_init = wandb.config.weight_init\n",
    "    act_fun = wandb.config.activation\n",
    "\n",
    "    nn_model = my_nn(epochs = epochs, nhl = nhl, sz = sz, w_d = w_d, lr = lr, optimizer = optimizer, b_sz = b_sz, weight_init = weight_init, act_fun = act_fun)\n",
    "    nn_model.train(x_train, y_train, x_valid, y_valid)\n",
    "    \n",
    "# wandb.agent(sweep_id, function = main, count = 100)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5aOW81WlMTQL"
   },
   "outputs": [],
   "source": [
    "# nhl = 5\n",
    "# sz = 128\n",
    "# act_fun = \"ReLU\"\n",
    "# weight_init = \"random\"\n",
    "# epochs = 5\n",
    "# lr = 1e-3\n",
    "# w_d = 0\n",
    "# loss = \"cross_entropy\"\n",
    "# b_sz = 64\n",
    "# mom = 0.9\n",
    "# epsilon = 0.000001\n",
    "# beta = 0.9\n",
    "# beta1 = 0.9\n",
    "# beta2 = 0.999\n",
    "# optimizer = \"nadam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9F4eGQS8MU7W"
   },
   "outputs": [],
   "source": [
    "# nn = my_nn(nhl=nhl, sz=sz, weight_init=weight_init, act_fun=act_fun, loss=loss, epochs=epochs, b_sz=b_sz, optimizer=optimizer, lr=lr, w_d=w_d, mom=mom, epsilon=epsilon, beta=beta, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvBBUXxnNqeq",
    "outputId": "149dd450-8053-4a98-efcf-8110c71bdc1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : train_loss = 0.37 valid_loss = 0.40, train accuracy = 86.28 valid_accuracy = 84.62\n",
      "epoch 2 : train_loss = 0.32 valid_loss = 0.36, train accuracy = 88.33 valid_accuracy = 86.98\n",
      "epoch 3 : train_loss = 0.29 valid_loss = 0.36, train accuracy = 89.02 valid_accuracy = 87.25\n",
      "epoch 4 : train_loss = 0.28 valid_loss = 0.35, train accuracy = 89.79 valid_accuracy = 87.57\n",
      "epoch 5 : train_loss = 0.26 valid_loss = 0.36, train accuracy = 90.23 valid_accuracy = 87.77\n"
     ]
    }
   ],
   "source": [
    "# nn.train(x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53D7LbzjLIg2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
